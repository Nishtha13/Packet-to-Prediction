# -*- coding: utf-8 -*-
"""Copy of Final code'

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zlPdntNyx9XlYiiNsO_yMsdblaJ46HPa
"""

# from google.colab import drive
# drive.mount('/content/drive')

import numpy as np
import pandas as pd

//Normal Sleep data from university environment
df1 = pd.read_csv('/content/distSleepDay3 With decimal.csv')
//Normal Sleep data from home environment
df2 = pd.read_csv('/content/normalSleepDay2 with decimals.csv')

df = pd.concat([df1, df2], ignore_index=True)

# Display the concatenated DataFrame
print(df)

df2

df2['timestamp'] = pd.to_datetime(df2['timestamp'], unit='s')
df2['timestamp'] = df2['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')

df2

import pandas as pd

# Assuming your DataFrame is named "df"


# Sort the DataFrame by timestamp
df = df.sort_values('timestamp')

# Group the DataFrame by event subtype
grouped = df.groupby('event subtype')

# Create a new column to store the corr_diff values
df['corr_diff'] = pd.NaT

# Iterate over each group
for _, group in grouped:
    # Get the difference between the current timestamp and the previous timestamp
    diff = group['timestamp'].diff()
    diff.iloc[0] = 0


    # Update the corr_diff column with the computed differences
    df.loc[group.index, 'corr_diff'] = diff

# Print the updated DataFrame
df

df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
df['timestamp'] = df['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')

df

import pandas as pd

# Assuming your DataFrame is named "df" and has the "timestamp" and "event subtype" columns

# Convert timestamp column to datetime if it's not already
df['timestamp'] = pd.to_datetime(df['timestamp'])

# Define the start and end timestamps for each hour interval
hour_intervals = [
    ("2022-07-13 19:59:41", "2022-07-13 20:59:41"),
    ("2022-07-13 20:59:42", "2022-07-13 21:59:41"),
    ("2022-07-13 21:59:42", "2022-07-13 22:59:41"),
    ("2022-07-13 22:59:42", "2022-07-13 23:59:41"),
    ("2022-07-13 23:59:42", "2022-07-14 00:59:41"),
    ("2022-07-14 00:59:42", "2022-07-14 01:59:41"),
    ("2022-07-14 01:59:42", "2022-07-14 02:59:41"),
    ("2022-07-14 02:59:42", "2022-07-14 04:21:17")
]

# Create an empty DataFrame to store the counts
counts_df = pd.DataFrame(columns=['hour', 'event_subtype', 'count'])

# Iterate over each hour interval
for start, end in hour_intervals:
    # Filter the DataFrame for the current hour interval
    hour_df = df[(df['timestamp'] >= start) & (df['timestamp'] <= end)]

    # Group the DataFrame by event subtype and calculate the count
    event_counts = hour_df['event subtype'].value_counts().reset_index()
    event_counts.columns = ['event_subtype', 'count']

    # Add the hour information to the event_counts DataFrame
    event_counts['hour'] = start

    # Append the counts to the counts_df DataFrame
    counts_df = counts_df.append(event_counts)

# Print the counts DataFrame
counts_df

new_df1 = df.copy()

new_df1['label'] = None

new_df1['label'] = 'Unknown'

awake_range_1 = pd.date_range("2022-07-13 19:59:40", "2022-07-13 20:59:41", freq='S')
sleep_range_1 = pd.date_range("2022-07-13 20:59:42", "2022-07-13 21:59:41", freq='S')
awake_range_2 = pd.date_range("2022-07-13 21:59:42", "2022-07-13 22:59:41", freq='S')
sleep_range_2 = pd.date_range("2022-07-13 22:59:42", "2022-07-13 23:59:41", freq='S')
awake_range_3 = pd.date_range("2022-07-13 23:59:42", "2022-07-14 00:59:41", freq='S')
sleep_range_3 = pd.date_range("2022-07-14 00:59:42", "2022-07-14 01:59:41", freq='S')
awake_range_4 = pd.date_range("2022-07-14 01:59:42", "2022-07-14 02:59:42", freq='S')
sleep_range_4 = pd.date_range("2022-07-14 02:59:42", "2022-07-14 04:21:17", freq='S')

awake_range_5 = pd.date_range("2022-07-07 19:30:07", "2022-07-08 03:30:05", freq='S')

new_df1.loc[new_df1['timestamp'].isin(awake_range_1), 'label'] = 'Sleep'
new_df1.loc[new_df1['timestamp'].isin(sleep_range_1), 'label'] = 'Awake'
new_df1.loc[new_df1['timestamp'].isin(awake_range_2), 'label'] = 'Sleep'
new_df1.loc[new_df1['timestamp'].isin(sleep_range_2), 'label'] = 'Awake'
new_df1.loc[new_df1['timestamp'].isin(awake_range_3), 'label'] = 'Sleep'
new_df1.loc[new_df1['timestamp'].isin(sleep_range_3), 'label'] = 'Awake'
new_df1.loc[new_df1['timestamp'].isin(awake_range_4), 'label'] = 'Sleep'
new_df1.loc[new_df1['timestamp'].isin(sleep_range_4), 'label'] = 'Awake'
new_df1.loc[new_df1['timestamp'].isin(awake_range_5), 'label'] = 'Sleep'

new_df1

# output_file = '/content/DistSleepDay2Labeled.xlsx'
# new_df1.to_excel(output_file, index=False)

unique_labels = new_df1['label'].unique()
print(unique_labels)

new_df1['label_ml'] = new_df1['label'].map({'Sleep': 0, 'Awake': 1})
new_df1

unique_labels = new_df1['label_ml'].value_counts()
print(unique_labels)

new_df1.corr(method="pearson")

X = new_df1.drop(['label_ml','timestamp','label','Source MAC address','RSSI'],axis=1)
Y = new_df1['label_ml']
new_df1 = new_df1.drop(['timestamp','label','Source MAC address','RSSI'],axis=1)

"""# New section"""

from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter

# Split the data into training and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)

#Apply undersampling to the training set
# undersampler = RandomUnderSampler()
# X_train_resampled, Y_train_resampled = undersampler.fit_resample(X_train, Y_train)

# # Count the classes after undersampling
# class_counts = Counter(Y_train_resampled)
# print("Class Counts after Undersampling:")
# for class_label, count in class_counts.items():
#     print(f"Class {class_label}: {count} samples")

# X_train = X_train_resampled
# Y_train = Y_train_resampled

# X_test = X_train
# Y_test = Y_train

Y

# from sklearn.model_selection import train_test_split
# from imblearn.under_sampling import RandomUnderSampler
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20)
# from sklearn.preprocessing import MinMaxScaler
# scaler = MinMaxScaler()
# fd = scaler.fit_transform(new_df1)
# fd = pd.DataFrame(fd)
# X_train = scaler.fit_transform(X_train)
# Y_test = Y_train
# X_test = X_train

# X_train

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from imblearn.pipeline import Pipeline
from  imblearn.over_sampling import SMOTE
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.svm import SVC
from sklearn.metrics import  precision_recall_fscore_support
from sklearn.neighbors import KNeighborsClassifier
from keras import backend as K
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import RadiusNeighborsClassifier

from sklearn.model_selection import KFold
cv = KFold(n_splits=5, random_state=1, shuffle=True)

steps = [('over', SMOTE()), ('model', KNeighborsClassifier())]
pipeline = Pipeline(steps=steps)
pipeline.fit(X_train,Y_train)

# scores = cross_val_score(pipeline, X, Y, cv=10, scoring='accuracy')
# print(scores)
# # print the mean and standard deviation of the scores
# print(f'Mean accuracy: {scores.mean():.3f}')
# print(f'Standard deviation: {scores.std():.3f}')

# from imblearn.under_sampling import RandomUnderSampler
# steps = [('under', RandomUnderSampler()), ('model',KNeighborsClassifier())]
# pipeline = Pipeline(steps)
# pipeline.fit(X_train, Y_train)
# y_pred = pipeline.predict(X_test)

predict = pipeline.predict(X_test)
predict

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
classification_report_result = classification_report(Y_test,predict)
print(classification_report_result)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming you have true labels (Y_test) and predicted probabilities (predict)
fpr, tpr, thresholds = roc_curve(Y_test, predict)
roc_auc = auc(fpr, tpr)

# Plotting the ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve -- KNN')
plt.legend(loc="lower right")
plt.show()

# #DistSleepDay3 and NormalSleepDay2 (using the same dataset as training)

# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_csv('/content/1st Hout.csv')
# dfh2 = pd.read_csv('/content/2nd Hour.csv')
# dfh3 = pd.read_csv('/content/3rd.csv')
# dfh4 = pd.read_csv('/content/4th hour.csv')
# dfh5 = pd.read_csv('/content/5th.csv')
# dfh6 = pd.read_csv('/content/6th hour.csv')
# dfh7 = pd.read_csv('/content/7th Hour.csv')
# dfh8 = pd.read_csv('/content/8th hour.csv')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()
# dfh4 = dfh4[['event subtype', 'corr_diff']].dropna()
# dfh5 = dfh5[['event subtype', 'corr_diff']].dropna()
# dfh6 = dfh6[['event subtype', 'corr_diff']].dropna()
# dfh7 = dfh7[['event subtype', 'corr_diff']].dropna()
# dfh8 = dfh8[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2, dfh3, dfh4, dfh5, dfh6, dfh7, dfh8]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)
#     dfh['Predicted Value'] = y_pred

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)

# #DistSleepDay2

# import pandas as pd
# import numpy as np

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_excel('/content/dfh1sleepday1(1).xlsx')
# dfh2 = pd.read_excel('/content/dfh2sleepday1(1).xlsx')
# dfh3 = pd.read_excel('/content/dfh3sleepday1(1).xlsx')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2,dfh3]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)

# # Print class_counts for each class
# for i, class_counts in enumerate(class_counts_array):
#     print(f"Class counts for prediction {outputs_array[i]}:")
#     for cls, count in class_counts.items():
#         print(f"Class {cls}: {count}")
#     print()

# #Normal_Sleepday1

# from sklearn.metrics import accuracy_score

# y_pred = outputs_array
# y_test = [0,0,0]

# accuracy = accuracy_score(y_test, y_pred)

# print("Accuracy:", accuracy)

# dfh1.head(10)

# #DistSleepDay3 and NormalSleepDay2 (using the same dataset as training)

# from sklearn.metrics import accuracy_score

# y_pred = outputs_array
# y_test = [0, 1, 0, 1, 0, 1, 0, 1]

# accuracy = accuracy_score(y_test, y_pred)

# print("Accuracy:", accuracy)

# #Normal_Sleep

# import pandas as pd
# import numpy as np

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_csv('/content/126 to 226.csv')
# dfh2 = pd.read_csv('/content/226 to 326.csv')
# dfh3 = pd.read_csv('/content/326 to 410.csv')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2, dfh3]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)
# # Print class_counts for each class
# for i, class_counts in enumerate(class_counts_array):
#     print(f"Class counts for prediction {outputs_array[i]}:")
#     for cls, count in class_counts.items():
#         print(f"Class {cls}: {count}")
#     print()

# #Normal_Sleep

# from sklearn.metrics import accuracy_score

# y_pred = outputs_array
# y_test = [0,0,0]

# accuracy = accuracy_score(y_test, y_pred)

# print("Accuracy:", accuracy)

# #DistSleepDay2

# import pandas as pd
# import numpy as np

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_excel('/content/dfh1new.xlsx')
# dfh2 = pd.read_excel('/content/dfh2new.xlsx')
# dfh3 = pd.read_excel('/content/dfh3new.xlsx')
# dfh4 = pd.read_excel('/content/dfh4new.xlsx')
# dfh5 = pd.read_excel('/content/dfh5new.xlsx')
# dfh6 = pd.read_excel('/content/dfh6new.xlsx')
# dfh7 = pd.read_excel('/content/dfh7new.xlsx')
# dfh8 = pd.read_excel('/content/dfh8new.xlsx')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()
# dfh4 = dfh4[['event subtype', 'corr_diff']].dropna()
# dfh5 = dfh5[['event subtype', 'corr_diff']].dropna()
# dfh6 = dfh6[['event subtype', 'corr_diff']].dropna()
# dfh7 = dfh7[['event subtype', 'corr_diff']].dropna()
# dfh8 = dfh8[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2, dfh3, dfh4, dfh5, dfh6, dfh7, dfh8]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)

# # Print class_counts for each class
# for i, class_counts in enumerate(class_counts_array):
#     print(f"Class counts for prediction {outputs_array[i]}:")
#     for cls, count in class_counts.items():
#         print(f"Class {cls}: {count}")
#     print()

# #DistSleepDay2

# from sklearn.metrics import accuracy_score

# y_pred = outputs_array
# y_test = [0, 1, 0, 1, 0, 1, 0, 1]

# accuracy = accuracy_score(y_test, y_pred)

# print("Accuracy:", accuracy)

steps = [('over', SMOTE()),('model',  RandomForestClassifier())]
pipeline = Pipeline(steps=steps)
pipeline.fit(X_train,Y_train)
# scores = cross_val_score(pipeline, X, Y, cv=10, scoring='accuracy')
# # print the mean and standard deviation of the scores
# print(f'Mean accuracy: {scores.mean():.3f}')
# print(f'Standard deviation: {scores.std():.3f}')

predict = pipeline.predict(X_test)
predict

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
classification_report_result = classification_report(Y_test,predict)
print(classification_report_result)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming you have true labels (Y_test) and predicted probabilities (predict)
fpr, tpr, thresholds = roc_curve(Y_test, predict)
roc_auc = auc(fpr, tpr)

# Plotting the ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve -- RF')
plt.legend(loc="lower right")
plt.show()

"""# New section"""

# #DistSleepDay2

# import pandas as pd
# import numpy as np

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_excel('/content/dfh1sleepday1(1).xlsx')
# dfh2 = pd.read_excel('/content/dfh2sleepday1(1).xlsx')
# dfh3 = pd.read_excel('/content/dfh3sleepday1(1).xlsx')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2,dfh3]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)

# # Print class_counts for each class
# for i, class_counts in enumerate(class_counts_array):
#     print(f"Class counts for prediction {outputs_array[i]}:")
#     for cls, count in class_counts.items():
#         print(f"Class {cls}: {count}")
#     print()

# #Normal_Sleepday2

# from sklearn.metrics import accuracy_score

# y_pred = outputs_array
# y_test = [0,0,0]

# accuracy = accuracy_score(y_test, y_pred)

# print("Accuracy:", accuracy)

# #DistSleepDay3 and NormalSleepDay2 (using the same dataset as training)

# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_csv('/content/1st Hout.csv')
# dfh2 = pd.read_csv('/content/2nd Hour.csv')
# dfh3 = pd.read_csv('/content/3rd.csv')
# dfh4 = pd.read_csv('/content/4th hour.csv')
# dfh5 = pd.read_csv('/content/5th.csv')
# dfh6 = pd.read_csv('/content/6th hour.csv')
# dfh7 = pd.read_csv('/content/7th Hour.csv')
# dfh8 = pd.read_csv('/content/8th hour.csv')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()
# dfh4 = dfh4[['event subtype', 'corr_diff']].dropna()
# dfh5 = dfh5[['event subtype', 'corr_diff']].dropna()
# dfh6 = dfh6[['event subtype', 'corr_diff']].dropna()
# dfh7 = dfh7[['event subtype', 'corr_diff']].dropna()
# dfh8 = dfh8[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2, dfh3, dfh4, dfh5, dfh6, dfh7, dfh8]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)
#     dfh['Predicted Value'] = y_pred

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)

# #DistSleepDay3 and NormalSleepDay2 (using the same dataset as training)

# from sklearn.metrics import accuracy_score

# y_pred = outputs_array
# y_test = [0, 1, 0, 1, 0, 1, 0, 1]

# accuracy = accuracy_score(y_test, y_pred)

# print("Accuracy:", accuracy)

# #Normal_Sleep

# import pandas as pd
# import numpy as np

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_csv('/content/126 to 226.csv')
# dfh2 = pd.read_csv('/content/226 to 326.csv')
# dfh3 = pd.read_csv('/content/326 to 410.csv')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2, dfh3]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)
# # Print class_counts for each class
# for i, class_counts in enumerate(class_counts_array):
#     print(f"Class counts for prediction {outputs_array[i]}:")
#     for cls, count in class_counts.items():
#         print(f"Class {cls}: {count}")
#     print()

# #Normal_Sleepday2

# from sklearn.metrics import accuracy_score

# y_pred = outputs_array
# y_test = [0,0,0]

# accuracy = accuracy_score(y_test, y_pred)

# print("Accuracy:", accuracy)

# #DistSleepDay2

# import pandas as pd
# import numpy as np

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_excel('/content/dfh1new.xlsx')
# dfh2 = pd.read_excel('/content/dfh2new.xlsx')
# dfh3 = pd.read_excel('/content/dfh3new.xlsx')
# dfh4 = pd.read_excel('/content/dfh4new.xlsx')
# dfh5 = pd.read_excel('/content/dfh5new.xlsx')
# dfh6 = pd.read_excel('/content/dfh6new.xlsx')
# dfh7 = pd.read_excel('/content/dfh7new.xlsx')
# dfh8 = pd.read_excel('/content/dfh8new.xlsx')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()
# dfh4 = dfh4[['event subtype', 'corr_diff']].dropna()
# dfh5 = dfh5[['event subtype', 'corr_diff']].dropna()
# dfh6 = dfh6[['event subtype', 'corr_diff']].dropna()
# dfh7 = dfh7[['event subtype', 'corr_diff']].dropna()
# dfh8 = dfh8[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2, dfh3, dfh4, dfh5, dfh6, dfh7, dfh8]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)

# # Print class_counts for each class
# for i, class_counts in enumerate(class_counts_array):
#     print(f"Class counts for prediction {outputs_array[i]}:")
#     for cls, count in class_counts.items():
#         print(f"Class {cls}: {count}")
#     print()

# #DistSleepDay2

# from sklearn.metrics import accuracy_score

# y_pred = outputs_array
# y_test = [0, 1, 0, 1, 0, 1, 0, 1]

# accuracy = accuracy_score(y_test, y_pred)

# print("Accuracy:", accuracy)

# steps = [('over', SMOTE()), ('model',SVC())]
# pipeline = Pipeline(steps=steps)
# pipeline.fit(X_train,Y_train)

# # scores = cross_val_score(pipeline, X, Y, cv=10, scoring='accuracy')
# # # print the mean and standard deviation of the scores
# # print(f'Mean accuracy: {scores.mean():.3f}')
# # print(f'Standard deviation: {scores.std():.3f}')

# # Make predictions on the testing data
# y_pred_train = pipeline.predict(X_train)
# y_pred_test = pipeline.predict(X_test)

# # Calculate and print the training accuracy
# train_accuracy = accuracy_score(Y_train, y_pred_train)
# print(f"Training Accuracy: {train_accuracy:.3f}")

# # Calculate and print the testing accuracy
# test_accuracy = accuracy_score(Y_test, y_pred_test)
# print(f"Testing Accuracy: {test_accuracy:.3f}")

# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# # Print the classification report
# classification_report_result = classification_report(Y_train, y_pred_train)
# print(classification_report_result)

# classification_report_result = classification_report(Y_test, y_pred_test)
# print(classification_report_result)

# import matplotlib.pyplot as plt
# from sklearn.metrics import roc_curve, roc_auc_score

# # Calculate predicted probabilities for training and testing data
# y_pred_train_prob = pipeline.decision_function(X_train)
# y_pred_test_prob = pipeline.decision_function(X_test)

# # Compute false positive rate, true positive rate, and thresholds for ROC curve
# fpr_train, tpr_train, thresholds_train = roc_curve(Y_train, y_pred_train_prob)
# fpr_test, tpr_test, thresholds_test = roc_curve(Y_test, y_pred_test_prob)

# # Compute AUC score for ROC curve
# auc_train = roc_auc_score(Y_train, y_pred_train_prob)
# auc_test = roc_auc_score(Y_test, y_pred_test_prob)

# # Plotting the ROC curve
# plt.figure()
# plt.plot(fpr_train, tpr_train, color='darkorange', label='Train ROC curve (area = %0.2f)' % auc_train)
# plt.plot(fpr_test, tpr_test, color='blue', label='Test ROC curve (area = %0.2f)' % auc_test)
# plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.05])
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('Receiver Operating Characteristic (ROC) Curve')
# plt.legend(loc="lower right")
# plt.show()

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

steps = [('over', SMOTE()), ('model',  LogisticRegression())]
pipeline = Pipeline(steps=steps)
pipeline.fit(X_train,Y_train)

# scores = cross_val_score(pipeline, X, Y, cv=10, scoring='accuracy')
# # print the mean and standard deviation of the scores
# print(f'Mean accuracy: {scores.mean():.3f}')
# print(f'Standard deviation: {scores.std():.3f}')

# Make predictions on the testing data
y_pred_train = pipeline.predict(X_train)
y_pred_test = pipeline.predict(X_test)

# Calculate and print the training accuracy
train_accuracy = accuracy_score(Y_train, y_pred_train)
print(f"Training Accuracy: {train_accuracy:.3f}")

# Calculate and print the testing accuracy
test_accuracy = accuracy_score(Y_test, y_pred_test)
print(f"Testing Accuracy: {test_accuracy:.3f}")


# Print the classification report
classification_report_result = classification_report(Y_train, y_pred_train)
print(classification_report_result)

# Print the classification report
classification_report_result = classification_report(Y_test, y_pred_test)
print(classification_report_result)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# Calculate predicted probabilities for testing data
y_pred_test_prob = pipeline.predict_proba(X_test)[:, 1]

# Compute false positive rate, true positive rate, and thresholds for ROC curve
fpr_test, tpr_test, thresholds_test = roc_curve(Y_test, y_pred_test_prob)

# Compute AUC score for ROC curve
auc_test = roc_auc_score(Y_test, y_pred_test_prob)

# Plotting the ROC curve for the test data
plt.figure()
plt.plot(fpr_test, tpr_test, color='darkorange', label='Test ROC curve (area = %0.2f)' % auc_test)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Test Data -- LR')
plt.legend(loc="lower right")
plt.show()

# #DistSleepDay2

# import pandas as pd
# import numpy as np

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_excel('/content/dfh1new.xlsx')
# dfh2 = pd.read_excel('/content/dfh2new.xlsx')
# dfh3 = pd.read_excel('/content/dfh3new.xlsx')
# dfh4 = pd.read_excel('/content/dfh4new.xlsx')
# dfh5 = pd.read_excel('/content/dfh5new.xlsx')
# dfh6 = pd.read_excel('/content/dfh6new.xlsx')
# dfh7 = pd.read_excel('/content/dfh7new.xlsx')
# dfh8 = pd.read_excel('/content/dfh8new.xlsx')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()
# dfh4 = dfh4[['event subtype', 'corr_diff']].dropna()
# dfh5 = dfh5[['event subtype', 'corr_diff']].dropna()
# dfh6 = dfh6[['event subtype', 'corr_diff']].dropna()
# dfh7 = dfh7[['event subtype', 'corr_diff']].dropna()
# dfh8 = dfh8[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2, dfh3, dfh4, dfh5, dfh6, dfh7, dfh8]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)

# # Print class_counts for each class
# for i, class_counts in enumerate(class_counts_array):
#     print(f"Class counts for prediction {outputs_array[i]}:")
#     for cls, count in class_counts.items():
#         print(f"Class {cls}: {count}")
#     print()#DistSleepDay2

# import pandas as pd
# import numpy as np

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_excel('/content/dfh1new.xlsx')
# dfh2 = pd.read_excel('/content/dfh2new.xlsx')
# dfh3 = pd.read_excel('/content/dfh3new.xlsx')
# dfh4 = pd.read_excel('/content/dfh4new.xlsx')
# dfh5 = pd.read_excel('/content/dfh5new.xlsx')
# dfh6 = pd.read_excel('/content/dfh6new.xlsx')
# dfh7 = pd.read_excel('/content/dfh7new.xlsx')
# dfh8 = pd.read_excel('/content/dfh8new.xlsx')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()
# dfh4 = dfh4[['event subtype', 'corr_diff']].dropna()
# dfh5 = dfh5[['event subtype', 'corr_diff']].dropna()
# dfh6 = dfh6[['event subtype', 'corr_diff']].dropna()
# dfh7 = dfh7[['event subtype', 'corr_diff']].dropna()
# dfh8 = dfh8[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2, dfh3, dfh4, dfh5, dfh6, dfh7, dfh8]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)

# # Print class_counts for each class
# for i, class_counts in enumerate(class_counts_array):
#     print(f"Class counts for prediction {outputs_array[i]}:")
#     for cls, count in class_counts.items():
#         print(f"Class {cls}: {count}")
#     print()

steps = [('over', SMOTE()), ('model', GradientBoostingClassifier())]
pipeline = Pipeline(steps=steps)
pipeline.fit(X_train,Y_train)

# scores = cross_val_score(pipeline, X, Y, cv=10, scoring='accuracy')
# # print the mean and standard deviation of the scores
# print(f'Mean accuracy: {scores.mean():.3f}')
# print(f'Standard deviation: {scores.std():.3f}')

predict = pipeline.predict(X_test)
predict

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
classification_report_result = classification_report(Y_test,predict)
print(classification_report_result)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# Calculate predicted probabilities for the test data
y_pred_prob = pipeline.predict_proba(X_test)[:, 1]

# Compute false positive rate, true positive rate, and thresholds for ROC curve
fpr, tpr, thresholds = roc_curve(Y_test, y_pred_prob)

# Compute AUC score for ROC curve
auc = roc_auc_score(Y_test, y_pred_prob)

# Plotting the ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve -- GB')
plt.legend(loc="lower right")
plt.show()

# #DistSleepDay2

# import pandas as pd
# import numpy as np

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_excel('/content/dfh1new.xlsx')
# dfh2 = pd.read_excel('/content/dfh2new.xlsx')
# dfh3 = pd.read_excel('/content/dfh3new.xlsx')
# dfh4 = pd.read_excel('/content/dfh4new.xlsx')
# dfh5 = pd.read_excel('/content/dfh5new.xlsx')
# dfh6 = pd.read_excel('/content/dfh6new.xlsx')
# dfh7 = pd.read_excel('/content/dfh7new.xlsx')
# dfh8 = pd.read_excel('/content/dfh8new.xlsx')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()
# dfh4 = dfh4[['event subtype', 'corr_diff']].dropna()
# dfh5 = dfh5[['event subtype', 'corr_diff']].dropna()
# dfh6 = dfh6[['event subtype', 'corr_diff']].dropna()
# dfh7 = dfh7[['event subtype', 'corr_diff']].dropna()
# dfh8 = dfh8[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2, dfh3, dfh4, dfh5, dfh6, dfh7, dfh8]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)

# # Print class_counts for each class
# for i, class_counts in enumerate(class_counts_array):
#     print(f"Class counts for prediction {outputs_array[i]}:")
#     for cls, count in class_counts.items():
#         print(f"Class {cls}: {count}")
#     print()

pipeline

# #Normalsleep1

# import pandas as pd
# import numpy as np

# def get_most_common_prediction(y_pred):
#     unique_values, counts = np.unique(y_pred, return_counts=True)
#     most_common_index = np.argmax(counts)
#     most_common_prediction = unique_values[most_common_index]
#     class_counts = dict(zip(unique_values, counts))
#     return most_common_prediction, class_counts

# # Read the DataFrames
# dfh1 = pd.read_excel('/content/dfh1sleepday1(1).xlsx')
# dfh2 = pd.read_excel('/content/dfh2sleepday1(1).xlsx')
# dfh3 = pd.read_excel('/content/dfh3sleepday1(1).xlsx')

# dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
# dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
# dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()

# # Create a list to store the outputs
# outputs = []
# class_counts_list = []

# # Perform prediction for each DataFrame
# for dfh in [dfh1, dfh2,dfh3]:
#     # Assuming you have prepared the features for each DataFrame
#     X_test = dfh  # Adjust this according to your DataFrame structure

#     # Perform the prediction
#     y_pred = pipeline.predict(X_test)

#     # Get the most common prediction and class counts
#     most_common_prediction, class_counts = get_most_common_prediction(y_pred)

#     # Append the output to the list
#     outputs.append((most_common_prediction))
#     class_counts_list.append(class_counts)

# # Convert the outputs list to an array
# outputs_array = np.array(outputs)
# class_counts_array = np.array(class_counts_list)

# # Print the array of outputs
# print(outputs_array)

# # Print class_counts for each class
# for i, class_counts in enumerate(class_counts_array):
#     print(f"Class counts for prediction {outputs_array[i]}:")
#     for cls, count in class_counts.items():
#         print(f"Class {cls}: {count}")
#     print()

#Normal_Sleepday1

from sklearn.metrics import accuracy_score

y_pred = outputs_array
y_test = [0,0,0]

accuracy = accuracy_score(y_test, y_pred)

print("Accuracy:", accuracy)

from keras.models import Sequential
from keras.layers import Dense
import matplotlib.pyplot as plt

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

X = new_df1[['event subtype', 'corr_diff']]
y = new_df1['label_ml']

X = np.array(X)
y = np.array(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

input_shape = (X_train.shape[1],)  # Remove the extra dimension

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# Apply normalization
X_train = X_train / 255.0
X_test = X_test / 255.0

y_train = y_train.astype('int32')

model = Sequential()
model.add(Dense(64, activation='relu', input_shape=input_shape))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])

history = model.fit(X_train, y_train, validation_split=0.5, epochs=10)

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)

print(history.history.keys())

y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5)  # Convert probabilities to binary predictions

# Assuming your target variable Y_test is one-hot encoded, you can convert it back to single-column labels
# Y_test_labels = Y_test.argmax(axis=1)
# y_pred_labels = y_pred.argmax(axis=1)

# Generate the classification report
report = classification_report(Y_test, y_pred)
print(report)

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

# import tensorflow as tf
# from tensorflow.keras import layers
# from sklearn.metrics import accuracy_score

# X = new_df1[['event subtype','corr_diff']]
# y = new_df1['label_ml']

# X = np.array(X)
# y = np.array(y)

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# input_shape = (X_train.shape[1], 1)  # Add an extra dimension for channels
# X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
# X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# X_train = X_train / 255.0
# X_test = X_test / 255.0

# X_train = X_train.astype('float32')
# y_train = y_train.astype('int32')

# model = tf.keras.Sequential([
#     layers.Conv1D(32, 1, activation='relu', input_shape=input_shape),
#     layers.MaxPooling1D(2),
#     layers.Flatten(),
#     layers.Dense(128, activation='relu'),
#     layers.Dense(1, activation='sigmoid')
# ])

# model.compile(optimizer='adam',
#               loss='binary_crossentropy',
#               metrics=['accuracy'])

# model.fit(X_train, y_train, epochs=100, batch_size=32)

# X_test = X_test.astype('float32')
# y_pred = model.predict(X_test)
# y_pred = y_pred.astype('float32')

# classes_x = np.argmax(y_pred, axis=1)
# y_test = y_test.reshape(-1)
# classes_x = classes_x.reshape(-1)

# accuracy = accuracy_score(y_test, classes_x)
# print("Accuracy:", accuracy)

# report = classification_report(y_test, classes_x)
# print(report)

from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras import layers
from sklearn.metrics import accuracy_score

X = new_df1[['event subtype', 'corr_diff']]
y = new_df1['label_ml']

X = np.array(X)
y = np.array(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

input_shape = (X_train.shape[1], 1)  # Add an extra dimension

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# Apply normalization
X_train = X_train / 255.0
X_test = X_test / 255.0

y_train = y_train.astype('int32')

# Apply undersampling
undersampler = RandomUnderSampler(random_state=42)
X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)

# Reshape the data for the model
X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], X_train_resampled.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Continue with model training using the resampled data
model = tf.keras.Sequential([
    layers.Conv1D(32, 1, activation='relu', input_shape=input_shape),
    layers.MaxPooling1D(2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])

history = model.fit(X_train_resampled, y_train_resampled, validation_data=(X_test, y_test), epochs=10)

# Generate predictions
y_pred = model.predict(X_test)
y_pred_classes = np.round(y_pred)
y_pred_classes_int = y_pred_classes.astype(int)

class_counts = np.bincount(y_train_resampled)
print("Class_counts",class_counts)

# Classification report
target_names = ['Class 0', 'Class 1']
report = classification_report(y_test, y_pred_classes_int, target_names=target_names)
print(report)

y_pred_classes_int = y_pred_classes.astype(int)
print(y_pred_classes_int)

unique_values = np.unique(y_pred_classes_int)

print(unique_values)

import numpy as np
from sklearn.metrics import roc_curve

# Assuming you have the following variables:
# y_true: the true class labels for the test set
# y_pred: an array of probability predictions from the CNN

# Calculate the false positive rate (fpr) and true positive rate (tpr)
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

# Find the optimal threshold that maximizes the true positive rate and minimizes the false positive rate
optimal_threshold = thresholds[np.argmax(tpr - fpr)]

print("Optimal Threshold:", optimal_threshold)

#DistSleepDay2

import pandas as pd
import numpy as np

def get_most_common_prediction(y_pred):
    unique_values, counts = np.unique(y_pred, return_counts=True)
    most_common_index = np.argmax(counts)
    most_common_prediction = unique_values[most_common_index]
    class_counts = dict(zip(unique_values, counts))
    return most_common_prediction, class_counts

# Read the DataFrames
dfh1 = pd.read_excel('/content/dfh1new.xlsx')
dfh2 = pd.read_excel('/content/dfh2new.xlsx')
dfh3 = pd.read_excel('/content/dfh3new.xlsx')
dfh4 = pd.read_excel('/content/dfh4new.xlsx')
dfh5 = pd.read_excel('/content/dfh5new.xlsx')
dfh6 = pd.read_excel('/content/dfh6new.xlsx')
dfh7 = pd.read_excel('/content/dfh7new.xlsx')
dfh8 = pd.read_excel('/content/dfh8new.xlsx')

dfh1 = dfh1[['event subtype', 'corr_diff']].dropna()
dfh2 = dfh2[['event subtype', 'corr_diff']].dropna()
dfh3 = dfh3[['event subtype', 'corr_diff']].dropna()
dfh4 = dfh4[['event subtype', 'corr_diff']].dropna()
dfh5 = dfh5[['event subtype', 'corr_diff']].dropna()
dfh6 = dfh6[['event subtype', 'corr_diff']].dropna()
dfh7 = dfh7[['event subtype', 'corr_diff']].dropna()
dfh8 = dfh8[['event subtype', 'corr_diff']].dropna()

# Create a list to store the outputs
outputs = []
class_counts_list = []

# Perform prediction for each DataFrame
for dfh in [dfh1, dfh2, dfh3, dfh4, dfh5, dfh6, dfh7, dfh8]:
    # Assuming you have prepared the features for each DataFrame
    X_test = dfh  # Adjust this according to your DataFrame structure

    # Perform the prediction
    y_pred = model.predict(X_test)
    y_pred_classes = np.where(y_pred >=  0.5 , 1, 0)

    # Get the most common prediction and class counts
    most_common_prediction, class_counts = get_most_common_prediction(y_pred_classes)

    # Append the output to the list
    outputs.append((most_common_prediction))
    class_counts_list.append(class_counts)

# Convert the outputs list to an array
outputs_array = np.array(outputs)
class_counts_array = np.array(class_counts_list)

# Print the array of outputs
print(outputs_array)

# Print class_counts for each class
for i, class_counts in enumerate(class_counts_array):
    print(f"Class counts for prediction {outputs_array[i]}:")
    for cls, count in class_counts.items():
        print(f"Class {cls}: {count}")
    print()

import graphviz
from IPython.display import Image, display

# Create the graph
graph = graphviz.Digraph(format='png')

# Add nodes
graph.node('Function', 'Function: \nGet Most Common Prediction')
graph.node('Dataset', 'Dataset: \nHourly Sleep Patterns')
graph.node('Disturbed Sleep Day 2', 'Disturbed Sleep Day 2\n(8 hours)')
graph.node('Normal Day 2', 'Normal Day 2\n(3 hours)')
graph.node('Disturbed Sleep Day 3', 'Disturbed Sleep Day 3\n(8 hours)')

# Add edges
graph.edge('Dataset', 'Function')
graph.edge('Function', 'Disturbed Sleep Day 2')
graph.edge('Function', 'Normal Day 2')
graph.edge('Function', 'Disturbed Sleep Day 3')

# Save and render the graph
graph.render('sleep_state_predictions', view=False)

# Display the graph
display(Image(filename='sleep_state_predictions.png'))
